{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "91653a4f-76ca-49c6-9a5b-1543ad489da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0000,  0.3118, -1.0000]])\n",
      "tensor([[0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import einx\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def genList(listSize):\n",
    "        return [(-1 if random.randint(0, 1) else random.random()) for x in range(listSize)]\n",
    "\n",
    "def genAnswerKey(inputList):\n",
    "    answerKeyList = []\n",
    "    for val in inputList:\n",
    "        numLessThanVal = 0\n",
    "        for otherVal in inputList:\n",
    "            if otherVal < val and otherVal > 0:\n",
    "                numLessThanVal += 1\n",
    "        answerKeyList.append(numLessThanVal)\n",
    "    return answerKeyList\n",
    "\n",
    "\n",
    "class ListDataset(Dataset):\n",
    "    def __init__(self, listSize, datasetSize):\n",
    "        self.datasetSize = datasetSize\n",
    "        self.listSize = listSize\n",
    "        self.allLists = [genList(self.listSize) for idx in range(self.datasetSize)]\n",
    "        self.allAnswerKeys = [genAnswerKey(inputList) for inputList in self.allLists]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datasetSize\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.allLists[idx]).float(), torch.tensor(self.allAnswerKeys[idx]).float()\n",
    "\n",
    "basicDataset = ListDataset(3, 5)\n",
    "sampleLoader = DataLoader(basicDataset, batch_size=1, shuffle=True)\n",
    "sampleInputTensor, sampleAnswerKey = next(iter(sampleLoader))\n",
    "print(sampleInputTensor)\n",
    "print(sampleAnswerKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "id": "18ca529a-8981-4463-9f80-12a2ab9d8683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Transform a (B, L) list to a (B, L, d_emb)\n",
    "class Stem(nn.Module):\n",
    "    def __init__(self, embeddingDim):\n",
    "        super().__init__()\n",
    "        self.linearToEmbDim = nn.Linear(1, embeddingDim)\n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "\n",
    "    # X expected to be a (B, L) list\n",
    "    def forward(self, x):\n",
    "        x = einx.rearrange(\"b c -> b c 1\", x)\n",
    "        x = self.linearToEmbDim(x)\n",
    "        x = self.gelu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "stem = Stem(4)\n",
    "sampleEmb = stem(sampleInputTensor)\n",
    "print(sampleEmb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "id": "84f01fad-edec-4e58-9457-0e253a7e4652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 1.],\n",
      "         [1., 0., 1.],\n",
      "         [1., 1., 2.]]])\n",
      "tensor([[[0.2327, 0.3837, 0.3837],\n",
      "         [0.3837, 0.2327, 0.3837],\n",
      "         [0.2741, 0.2741, 0.4519]]])\n",
      "tensor([[[1.5471, 0.6163],\n",
      "         [2.3019, 0.6163],\n",
      "         [1.8222, 0.5481]]])\n"
     ]
    }
   ],
   "source": [
    "keyDim = 2\n",
    "\n",
    "sampleQuery = torch.tensor([\n",
    "    [\n",
    "        [0., 1],\n",
    "        [1, 0],\n",
    "        [1, 1]\n",
    "    ]\n",
    "])\n",
    "\n",
    "sampleKey = torch.tensor([\n",
    "    [\n",
    "        [1., 0],\n",
    "        [0, 1],\n",
    "        [1, 1]\n",
    "    ]\n",
    "])\n",
    "\n",
    "# Perform attention from all queries to all keys\n",
    "dotProd = einx.dot(\"b q [d], b k [d] -> b q k\", sampleQuery, sampleKey)\n",
    "print(dotProd)\n",
    "\n",
    "softMaxPerQuery = einx.softmax(\"b q [k]\", dotProd / keyDim)\n",
    "print(softMaxPerQuery)\n",
    "\n",
    "sampleValDeltas = torch.tensor([\n",
    "    [\n",
    "        [5., 1],\n",
    "        [0, 1],\n",
    "        [1, 0]\n",
    "    ]\n",
    "])\n",
    "\n",
    "# Each value is associated with the key that outputted that value so we do a matrix multiply across that key dimension to get the weighted sums \n",
    "# of each value embedding for each query:\n",
    "appliedAttentionWeights = einx.dot(\"b q [k], b [k] h -> b q h\", softMaxPerQuery, sampleValDeltas)\n",
    "\n",
    "print(appliedAttentionWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "id": "286e8727-6566-4753-9d79-8962aacb19a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0165,  0.0198, -0.0152,  0.0110],\n",
       "         [ 0.0164,  0.0198, -0.0152,  0.0110],\n",
       "         [ 0.0165,  0.0198, -0.0152,  0.0110]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 738,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import einx.nn.torch as einn\n",
    "\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, emb_dim, head_size):\n",
    "        super().__init__()\n",
    "        self.keyMap = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.queryMap = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.valueDownMap = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.valueUpMap = nn.Linear(head_size, emb_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (B, C, emb_dim)\n",
    "        key = self.keyMap(x) # (B, C, head_size)\n",
    "        query = self.queryMap(x) # (B, C, head_size)\n",
    "        valueDown = self.valueDownMap(x) # (B, C, head_size)\n",
    "\n",
    "        # Self-attention, pairwise dot-prod all queries to all keys then softmax\n",
    "        attentionDotProd = einx.dot(\"b q [d], b k [d] -> b q k\", query, key) # (B, C, C)\n",
    "        softMaxPerQuery = einx.softmax(\"b q [k]\", attentionDotProd / np.sqrt(keyDim))\n",
    "        # Take the weighted sum of the valueDown heads for each channel, where each channel corresponds to \n",
    "        appliedAttentionWeights = einx.dot(\"b q [k], b [k] h -> b q h\", softMaxPerQuery, valueDown) # (B C head_size)\n",
    "\n",
    "        embDelta = self.valueUpMap(appliedAttentionWeights) # (B, C, emb_dim)\n",
    "\n",
    "        return embDelta\n",
    "\n",
    "emb_dim = 4\n",
    "batchSize = 1\n",
    "numChannels = 3\n",
    "head_size = 2\n",
    "\n",
    "atHead = AttentionHead(emb_dim, head_size)\n",
    "atHead(sampleEmb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "id": "8e7d186f-aada-4ae4-a93c-4604ebc6dab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0347,  0.4977,  1.3873, -0.8503],\n",
       "         [-1.0434,  0.5081,  1.3803, -0.8450],\n",
       "         [-1.0347,  0.4977,  1.3873, -0.8503]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 739,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = emb_dim // num_heads\n",
    "        self.attention_heads = nn.ModuleList([AttentionHead(emb_dim, self.head_size) for _ in range(num_heads)])\n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        valueDelta = torch.zeros_like(x)\n",
    "        for head in self.attention_heads:\n",
    "            valueDelta += head(x)\n",
    "        return self.norm(valueDelta)\n",
    "\n",
    "num_heads = 2\n",
    "multiHeadedAtt = MultiHeadedAttention(emb_dim, num_heads)\n",
    "multiHeadedAtt(sampleEmb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "id": "c1717ecc-6ac7-44fb-93f0-ff94233a1cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0568,  1.6622, -0.2668, -1.9766],\n",
       "         [ 0.1508,  1.8950, -0.3871, -2.0002],\n",
       "         [-0.0568,  1.6622, -0.2668, -1.9766]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 740,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(emb_dim, num_heads)\n",
    "        self.feedforward_nonlinear = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim * 4, emb_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(emb_dim)\n",
    "        self.norm2 = nn.LayerNorm(emb_dim)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(x)\n",
    "        x = x + self.feedforward_nonlinear(x)\n",
    "        return x\n",
    "\n",
    "block = TransformerBlock(emb_dim, num_heads)\n",
    "block(sampleEmb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "id": "2a1055f9-ba56-4761-994c-3f02572d51e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 741,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim = 64\n",
    "num_heads = 8\n",
    "num_blocks = 6\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stem = Stem(emb_dim)\n",
    "\n",
    "        self.transformerBlocks = nn.ModuleList([TransformerBlock(emb_dim, num_heads) for _ in range(num_blocks)])\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        for block in self.transformerBlocks:\n",
    "            x = block(x)\n",
    "        x = self.output(x)\n",
    "        return x.squeeze(-1)\n",
    "        \n",
    "sampleInputTensor = next(iter(sampleLoader))[0]\n",
    "sampleTransformer = Transformer()\n",
    "sampleTransformer(sampleInputTensor).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "id": "8288e710-9a00-4071-9a60-0eb230b2b750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "transformer = Transformer()\n",
    "\n",
    "trainDataset = ListDataset(10, 1000)\n",
    "trainLoader = DataLoader(trainDataset, batch_size=64, shuffle=True)\n",
    "testDataset = ListDataset(10, 1000)\n",
    "testLoader = DataLoader(testDataset, batch_size=256, shuffle=True)\n",
    "\n",
    "inputTensor, targetTensor = next(iter(trainLoader))\n",
    "print(inputTensor.shape)\n",
    "transformer(inputTensor).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "id": "9ba01794-59d7-4210-8736-fb0df695a4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  Train Loss 2.826    TestLoss 0.95     \n",
      "Epoch  10 Train Loss 0.224    TestLoss 0.16     \n",
      "Epoch  20 Train Loss 0.111    TestLoss 0.1      \n",
      "Epoch  30 Train Loss 0.084    TestLoss 0.08     \n",
      "Epoch  40 Train Loss 0.063    TestLoss 0.06     \n",
      "Epoch  50 Train Loss 0.049    TestLoss 0.05     \n",
      "Epoch  60 Train Loss 0.027    TestLoss 0.03     \n",
      "Epoch  70 Train Loss 0.021    TestLoss 0.02     \n",
      "Epoch  80 Train Loss 0.016    TestLoss 0.02     \n",
      "Epoch  90 Train Loss 0.014    TestLoss 0.02     \n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "n_epochs = 100\n",
    "initialLearningRate = 0.003\n",
    "\n",
    "model = Transformer()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = initialLearningRate)\n",
    "scheduler = CosineAnnealingLR(optimizer, n_epochs)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    runningLoss = 0\n",
    "    for inputTensor, targetTensor in trainLoader:\n",
    "        model.zero_grad()\n",
    "        pred = model(inputTensor.to(device))\n",
    "        loss = criterion(pred, targetTensor.to(device))\n",
    "        loss.backward()\n",
    "        runningLoss += loss.item() * inputTensor.shape[0]\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    runningTestLoss = 0\n",
    "    for inputTensor, targetTensor in testLoader:\n",
    "        with torch.no_grad():\n",
    "            pred = model(inputTensor.to(device))\n",
    "            loss = criterion(pred, targetTensor.to(device))\n",
    "            runningTestLoss += loss.item() * inputTensor.shape[0]\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:< 4}Train Loss{round(runningLoss / len(trainDataset), 3):< 10}TestLoss{round(runningTestLoss / len(testDataset), 2):< 10}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "id": "2d321823-a09d-453c-9206-ae7275c4df22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input List:\n",
      "-1.00\t0.18\t0.02\t-1.00\t0.94\t0.50\t0.46\t0.15\t-1.00\t-1.00\n",
      "True Target List:\n",
      "0.00\t2.00\t0.00\t0.00\t5.00\t4.00\t3.00\t1.00\t0.00\t0.00\n",
      "Predicted List:\n",
      "0.00\t2.02\t-0.00\t0.00\t4.99\t4.00\t3.00\t0.98\t0.00\t0.00\n",
      "\n",
      "\n",
      "An In Distribution Input:\n",
      "0.10\t0.20\t0.30\t0.40\t0.50\t-1.00\t-1.00\t-1.00\t-1.00\t-1.00\n",
      "True Target:\n",
      "0.00\t1.00\t2.00\t3.00\t4.00\t0.00\t0.00\t0.00\t0.00\t0.00\n",
      "Predictd List:\n",
      "0.00\t0.99\t1.99\t2.98\t4.00\t0.00\t0.00\t0.00\t0.00\t0.00\n",
      "\n",
      "\n",
      "An Out Of Distribution Input:\n",
      "0.50\t0.60\t0.70\t0.80\t0.90\t0.95\t0.97\t-1.00\t-1.00\t-1.00\n",
      "True Target:\n",
      "0.00\t1.00\t2.00\t3.00\t4.00\t5.00\t6.00\t0.00\t0.00\t0.00\n",
      "Predictd List:\n",
      "-0.00\t0.98\t1.98\t3.05\t4.03\t4.95\t6.03\t0.00\t0.00\t0.00\n",
      "\n",
      "\n",
      "A Pathological Input:\n",
      "tensor([0.9000, 0.9900, 0.9990, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "True Target:\n",
      "0.00\t1.00\t2.00\t3.00\t4.00\t5.00\t6.00\t7.00\t8.00\t9.00\n",
      "Predictd List:\n",
      "0.87\t4.82\t5.71\t5.79\t5.79\t5.79\t5.79\t5.79\t5.79\t5.79\n"
     ]
    }
   ],
   "source": [
    "inp, t = next(iter(trainLoader))\n",
    "def parseTensor(tensor):\n",
    "    return '\\t'.join([f\"{x.item():.2f}\" for x in tensor])\n",
    "print(\"Input List:\")\n",
    "print(parseTensor(inp[0]))\n",
    "print(\"True Target List:\")\n",
    "print(parseTensor(t[0]))\n",
    "print(\"Predicted List:\")\n",
    "print(parseTensor(model(inp.to(device))[0]))\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "inp = torch.tensor([[0.1, 0.2, 0.3, 0.4, 0.5, -1, -1, -1, -1, -1]])\n",
    "trueTarget = torch.tensor([0, 1, 2, 3, 4, 0, 0, 0, 0, 0])\n",
    "print(\"An In Distribution Input:\")\n",
    "print(parseTensor(inp[0]))\n",
    "print(\"True Target:\")\n",
    "print(parseTensor(trueTarget))\n",
    "print(\"Predictd List:\")\n",
    "print(parseTensor(model(inp.to(device))[0]))\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "inp = torch.tensor([[0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.97, -1, -1, -1]])\n",
    "trueTarget = torch.tensor([0, 1, 2, 3, 4, 5, 6, 0, 0, 0])\n",
    "print(\"An Out Of Distribution Input:\")\n",
    "print(parseTensor(inp[0]))\n",
    "print(\"True Target:\")\n",
    "print(parseTensor(trueTarget))\n",
    "print(\"Predictd List:\")\n",
    "print(parseTensor(model(inp.to(device))[0]))\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "inp = torch.tensor([[0.9, 0.99, 0.999, 0.9999, 0.99999, 0.999999, 0.9999999, 0.99999999, 0.999999999, 0.9999999999]])\n",
    "trueTarget = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "print(\"A Pathological Input:\")\n",
    "print(inp[0])\n",
    "print(\"True Target:\")\n",
    "print(parseTensor(trueTarget))\n",
    "print(\"Predictd List:\")\n",
    "print(parseTensor(model(inp.to(device))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "id": "1434b4d6-cd2a-4957-8578-03cf47f19de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0     Train Loss 1.043    TestLoss 0.9      \n",
      "Epoch  50    Train Loss 0.47     TestLoss 0.5      \n",
      "Epoch  100   Train Loss 0.441    TestLoss 0.5      \n",
      "Epoch  150   Train Loss 0.42     TestLoss 0.5      \n",
      "Epoch  200   Train Loss 0.41     TestLoss 0.49     \n",
      "Epoch  250   Train Loss 0.396    TestLoss 0.49     \n",
      "Epoch  300   Train Loss 0.381    TestLoss 0.48     \n",
      "Epoch  350   Train Loss 0.371    TestLoss 0.48     \n",
      "Epoch  400   Train Loss 0.361    TestLoss 0.48     \n",
      "Epoch  450   Train Loss 0.354    TestLoss 0.48     \n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "class DumbModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dumbLin = nn.Sequential(\n",
    "            nn.Linear(10, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64,8),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(8,10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.dumbLin(x)\n",
    "\n",
    "n_epochs = 500\n",
    "initialLearningRate = 0.03\n",
    "\n",
    "dumbModel = DumbModel()\n",
    "\n",
    "optimizer = torch.optim.Adam(dumbModel.parameters(), lr = initialLearningRate)\n",
    "scheduler = CosineAnnealingLR(optimizer, n_epochs)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dumbModel.to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    runningLoss = 0\n",
    "    for inputTensor, targetTensor in trainLoader:\n",
    "        dumbModel.zero_grad()\n",
    "        pred = dumbModel(inputTensor.to(device))\n",
    "        loss = criterion(pred, targetTensor.to(device))\n",
    "        loss.backward()\n",
    "        runningLoss += loss.item() * inputTensor.shape[0]\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    runningTestLoss = 0\n",
    "    for inputTensor, targetTensor in testLoader:\n",
    "        with torch.no_grad():\n",
    "            pred = dumbModel(inputTensor.to(device))\n",
    "            loss = criterion(pred, targetTensor.to(device))\n",
    "            runningTestLoss += loss.item() * inputTensor.shape[0]\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch:< 7}Train Loss{round(runningLoss / len(trainDataset), 3):< 10}TestLoss{round(runningTestLoss / len(testDataset), 2):< 10}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "id": "269c1a0a-dc1c-459a-9395-bb0cf8d92efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input List:\n",
      "-1.00\t-1.00\t0.53\t0.55\t-1.00\t0.34\t0.92\t-1.00\t-1.00\t0.18\n",
      "True Target List:\n",
      "0.00\t0.00\t2.00\t3.00\t0.00\t1.00\t4.00\t0.00\t0.00\t0.00\n",
      "Predicted List:\n",
      "-0.02\t0.00\t2.00\t3.08\t-0.00\t1.01\t3.66\t0.00\t-0.02\t0.00\n",
      "\n",
      "\n",
      "An In Distribution Input:\n",
      "0.10\t0.20\t0.30\t0.40\t0.50\t-1.00\t-1.00\t-1.00\t-1.00\t-1.00\n",
      "True Target:\n",
      "0.00\t1.00\t2.00\t3.00\t4.00\t0.00\t0.00\t0.00\t0.00\t0.00\n",
      "Predictd List:\n",
      "0.21\t0.00\t2.00\t2.59\t2.37\t-0.04\t-0.00\t-0.08\t-0.02\t0.00\n",
      "\n",
      "\n",
      "An Out Of Distribution Input:\n",
      "0.50\t0.60\t0.70\t0.80\t0.90\t0.95\t0.97\t-1.00\t-1.00\t-1.00\n",
      "True Target:\n",
      "0.00\t1.00\t2.00\t3.00\t4.00\t5.00\t6.00\t0.00\t0.00\t0.00\n",
      "Predictd List:\n",
      "-0.38\t0.00\t4.51\t3.85\t6.48\t4.26\t4.03\t-0.03\t0.55\t0.00\n"
     ]
    }
   ],
   "source": [
    "inp, t = next(iter(trainLoader))\n",
    "def parseTensor(tensor):\n",
    "    return '\\t'.join([f\"{x.item():.2f}\" for x in tensor])\n",
    "print(\"Input List:\")\n",
    "print(parseTensor(inp[0]))\n",
    "print(\"True Target List:\")\n",
    "print(parseTensor(t[0]))\n",
    "print(\"Predicted List:\")\n",
    "print(parseTensor(dumbModel(inp.to(device))[0]))\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "inp = torch.tensor([[0.1, 0.2, 0.3, 0.4, 0.5, -1, -1, -1, -1, -1]])\n",
    "trueTarget = torch.tensor([0, 1, 2, 3, 4, 0, 0, 0, 0, 0])\n",
    "print(\"An In Distribution Input:\")\n",
    "print(parseTensor(inp[0]))\n",
    "print(\"True Target:\")\n",
    "print(parseTensor(trueTarget))\n",
    "print(\"Predictd List:\")\n",
    "print(parseTensor(dumbModel(inp.to(device))[0]))\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "inp = torch.tensor([[0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.97, -1, -1, -1]])\n",
    "trueTarget = torch.tensor([0, 1, 2, 3, 4, 5, 6, 0, 0, 0])\n",
    "print(\"An Out Of Distribution Input:\")\n",
    "print(parseTensor(inp[0]))\n",
    "print(\"True Target:\")\n",
    "print(parseTensor(trueTarget))\n",
    "print(\"Predictd List:\")\n",
    "print(parseTensor(dumbModel(inp.to(device))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bae1ad-4186-4998-b363-58183932b2f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb50a9-b590-43c1-b23e-4a0135804612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
