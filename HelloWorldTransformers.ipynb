{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 968,
   "id": "91653a4f-76ca-49c6-9a5b-1543ad489da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0000,  0.7272,  0.9370]])\n",
      "tensor([[0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import einx\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def genList(listSize):\n",
    "        return [(-1 if random.randint(0, 1) else random.random()) for x in range(listSize)]\n",
    "\n",
    "def genAnswerKey(inputList):\n",
    "    answerKeyList = []\n",
    "    for val in inputList:\n",
    "        numLessThanVal = 0\n",
    "        for otherVal in inputList:\n",
    "            if otherVal < val and otherVal > 0:\n",
    "                numLessThanVal += 1\n",
    "        answerKeyList.append(numLessThanVal)\n",
    "    return answerKeyList\n",
    "\n",
    "\n",
    "class ListDataset(Dataset):\n",
    "    def __init__(self, listSize, datasetSize):\n",
    "        self.datasetSize = datasetSize\n",
    "        self.listSize = listSize\n",
    "        self.allLists = [genList(self.listSize) for idx in range(self.datasetSize)]\n",
    "        self.allAnswerKeys = [genAnswerKey(inputList) for inputList in self.allLists]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datasetSize\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.allLists[idx]).float(), torch.tensor(self.allAnswerKeys[idx]).float()\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "basicDataset = ListDataset(3, 100)\n",
    "sampleLoader = DataLoader(basicDataset, batch_size=1, shuffle=True)\n",
    "sampleInputTensor, sampleAnswerKey = next(iter(sampleLoader))\n",
    "print(sampleInputTensor)\n",
    "print(sampleAnswerKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "id": "18ca529a-8981-4463-9f80-12a2ab9d8683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Transform a (B, L) list to a (B, L, d_emb)\n",
    "class Stem(nn.Module):\n",
    "    def __init__(self, embeddingDim):\n",
    "        super().__init__()\n",
    "        self.norm = einn.Norm(\"b [c]\")\n",
    "        self.linearToEmbDim = nn.Linear(1, embeddingDim)\n",
    "        self.gelu = nn.GELU()\n",
    "        \n",
    "\n",
    "    # X expected to be a (B, L) list\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = einx.rearrange(\"b c -> b c 1\", x)\n",
    "        x = self.linearToEmbDim(x)\n",
    "        x = self.gelu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "stem = Stem(4)\n",
    "sampleEmb = stem(sampleInputTensor)\n",
    "print(sampleEmb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "id": "84f01fad-edec-4e58-9457-0e253a7e4652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 1.],\n",
      "         [1., 0., 1.],\n",
      "         [1., 1., 2.]]])\n",
      "tensor([[[0.2327, 0.3837, 0.3837],\n",
      "         [0.3837, 0.2327, 0.3837],\n",
      "         [0.2741, 0.2741, 0.4519]]])\n",
      "tensor([[[0.8490, 0.3837],\n",
      "         [1.1510, 0.2327],\n",
      "         [1.0000, 0.2741]]])\n"
     ]
    }
   ],
   "source": [
    "keyDim = 2\n",
    "\n",
    "sampleQuery = torch.tensor([\n",
    "    [\n",
    "        [0., 1],\n",
    "        [1, 0],\n",
    "        [1, 1]\n",
    "    ]\n",
    "])\n",
    "\n",
    "sampleKey = torch.tensor([\n",
    "    [\n",
    "        [1., 0],\n",
    "        [0, 1],\n",
    "        [1, 1]\n",
    "    ]\n",
    "])\n",
    "\n",
    "# Perform attention from all queries to all keys\n",
    "dotProd = einx.dot(\"b q [d], b k [d] -> b q k\", sampleQuery, sampleKey)\n",
    "print(dotProd)\n",
    "\n",
    "softMaxPerQuery = einx.softmax(\"b q [k]\", dotProd / keyDim)\n",
    "print(softMaxPerQuery)\n",
    "\n",
    "sampleValDeltas = torch.tensor([\n",
    "    [\n",
    "        [2., 0],\n",
    "        [0, 1],\n",
    "        [1, 0]\n",
    "    ]\n",
    "])\n",
    "\n",
    "# Each value is associated with the key that outputted that value so we do a matrix multiply across that key dimension to get the weighted sums \n",
    "# of each value embedding for each query:\n",
    "appliedAttentionWeights = einx.dot(\"b q [k], b [k] h -> b q h\", softMaxPerQuery, sampleValDeltas)\n",
    "\n",
    "print(appliedAttentionWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "id": "286e8727-6566-4753-9d79-8962aacb19a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0556, -0.0076,  0.0241,  0.0462],\n",
       "         [-0.0562, -0.0074,  0.0243,  0.0465],\n",
       "         [-0.0565, -0.0074,  0.0244,  0.0467]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 971,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import einx.nn.torch as einn\n",
    "\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, emb_dim, head_size):\n",
    "        super().__init__()\n",
    "        self.keyMap = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.queryMap = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.valueDownMap = nn.Linear(emb_dim, head_size, bias=False)\n",
    "        self.valueUpMap = nn.Linear(head_size, emb_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (B, C, emb_dim)\n",
    "        key = self.keyMap(x) # (B, C, head_size)\n",
    "        query = self.queryMap(x) # (B, C, head_size)\n",
    "        valueDown = self.valueDownMap(x) # (B, C, head_size)\n",
    "\n",
    "        # Self-attention, pairwise dot-prod all queries to all keys then softmax\n",
    "        attentionDotProd = einx.dot(\"b q [d], b k [d] -> b q k\", query, key) # (B, C, C)\n",
    "        softMaxPerQuery = einx.softmax(\"b q [k]\", attentionDotProd / np.sqrt(keyDim))\n",
    "        # Take the weighted sum of the valueDown heads for each channel, where each channel corresponds to \n",
    "        appliedAttentionWeights = einx.dot(\"b q [k], b [k] h -> b q h\", softMaxPerQuery, valueDown) # (B C head_size)\n",
    "\n",
    "        embDelta = self.valueUpMap(appliedAttentionWeights) # (B, C, emb_dim)\n",
    "\n",
    "        return embDelta\n",
    "\n",
    "emb_dim = 4\n",
    "batchSize = 1\n",
    "numChannels = 3\n",
    "head_size = 2\n",
    "\n",
    "atHead = AttentionHead(emb_dim, head_size)\n",
    "atHead(sampleEmb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "id": "8e7d186f-aada-4ae4-a93c-4604ebc6dab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.5126, -1.2724, -0.2850,  0.0449],\n",
       "         [ 1.5051, -1.2824, -0.2778,  0.0550],\n",
       "         [ 1.5009, -1.2877, -0.2746,  0.0613]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 972,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = emb_dim // num_heads\n",
    "        self.attention_heads = nn.ModuleList([AttentionHead(emb_dim, self.head_size) for _ in range(num_heads)])\n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        valueDelta = torch.zeros_like(x)\n",
    "        for head in self.attention_heads:\n",
    "            valueDelta += head(x)\n",
    "        return self.norm(valueDelta)\n",
    "\n",
    "num_heads = 2\n",
    "multiHeadedAtt = MultiHeadedAttention(emb_dim, num_heads)\n",
    "multiHeadedAtt(sampleEmb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "id": "c1717ecc-6ac7-44fb-93f0-ff94233a1cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1220, -0.1524,  1.0363, -1.5487],\n",
       "         [ 0.6964,  0.1712,  1.0027, -1.2601],\n",
       "         [ 0.6674,  0.2180,  0.9976, -1.1155]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 973,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(emb_dim, num_heads)\n",
    "        self.feedforward_nonlinear = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim * 4, emb_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(x)\n",
    "        x = x + self.feedforward_nonlinear(x)\n",
    "        return x\n",
    "\n",
    "block = TransformerBlock(emb_dim, num_heads)\n",
    "block(sampleEmb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "id": "2a1055f9-ba56-4761-994c-3f02572d51e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 974,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim = 64\n",
    "num_heads = 8\n",
    "num_blocks = 6\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stem = Stem(emb_dim)\n",
    "\n",
    "        self.transformerBlocks = nn.ModuleList([TransformerBlock(emb_dim, num_heads) for _ in range(num_blocks)])\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        for block in self.transformerBlocks:\n",
    "            x = block(x)\n",
    "        x = self.output(x)\n",
    "        return x.squeeze(-1)\n",
    "        \n",
    "sampleInputTensor = next(iter(sampleLoader))[0]\n",
    "sampleTransformer = Transformer()\n",
    "sampleTransformer(sampleInputTensor).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "id": "8288e710-9a00-4071-9a60-0eb230b2b750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 975,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "transformer = Transformer()\n",
    "\n",
    "trainDataset = ListDataset(10, 1000)\n",
    "trainLoader = DataLoader(trainDataset, batch_size=64, shuffle=True)\n",
    "testDataset = ListDataset(10, 1000)\n",
    "testLoader = DataLoader(testDataset, batch_size=256, shuffle=True)\n",
    "\n",
    "inputTensor, targetTensor = next(iter(trainLoader))\n",
    "print(inputTensor.shape)\n",
    "transformer(inputTensor).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "id": "9ba01794-59d7-4210-8736-fb0df695a4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  Train Loss 2.012    TestLoss 0.91     \n",
      "Epoch  10 Train Loss 0.169    TestLoss 0.18     \n",
      "Epoch  20 Train Loss 0.099    TestLoss 0.12     \n",
      "Epoch  30 Train Loss 0.076    TestLoss 0.17     \n",
      "Epoch  40 Train Loss 0.066    TestLoss 0.05     \n",
      "Epoch  50 Train Loss 0.048    TestLoss 0.05     \n",
      "Epoch  60 Train Loss 0.028    TestLoss 0.03     \n",
      "Epoch  70 Train Loss 0.018    TestLoss 0.02     \n",
      "Epoch  80 Train Loss 0.015    TestLoss 0.02     \n",
      "Epoch  90 Train Loss 0.012    TestLoss 0.01     \n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "n_epochs = 100\n",
    "initialLearningRate = 0.003\n",
    "\n",
    "model = Transformer()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = initialLearningRate)\n",
    "scheduler = CosineAnnealingLR(optimizer, n_epochs)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    runningLoss = 0\n",
    "    for inputTensor, targetTensor in trainLoader:\n",
    "        model.zero_grad()\n",
    "        pred = model(inputTensor.to(device))\n",
    "        loss = criterion(pred, targetTensor.to(device))\n",
    "        loss.backward()\n",
    "        runningLoss += loss.item() * inputTensor.shape[0]\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    runningTestLoss = 0\n",
    "    for inputTensor, targetTensor in testLoader:\n",
    "        with torch.no_grad():\n",
    "            pred = model(inputTensor.to(device))\n",
    "            loss = criterion(pred, targetTensor.to(device))\n",
    "            runningTestLoss += loss.item() * inputTensor.shape[0]\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:< 4}Train Loss{round(runningLoss / len(trainDataset), 3):< 10}TestLoss{round(runningTestLoss / len(testDataset), 2):< 10}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "id": "2d321823-a09d-453c-9206-ae7275c4df22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input List:\n",
      "-1.00\t0.38\t0.77\t-1.00\t0.45\t0.28\t-1.00\t-1.00\t0.87\t0.31\n",
      "True Target List:\n",
      "0.00\t2.00\t4.00\t0.00\t3.00\t0.00\t0.00\t0.00\t5.00\t1.00\n",
      "Predicted List:\n",
      "0.00\t1.99\t4.00\t0.00\t3.02\t-0.00\t-0.00\t0.00\t5.01\t1.01\n",
      "\n",
      "\n",
      "An In Distribution Input:\n",
      "0.10\t0.20\t0.30\t0.40\t0.50\t-1.00\t-1.00\t-1.00\t-1.00\t-1.00\n",
      "True Target:\n",
      "0.00\t1.00\t2.00\t3.00\t4.00\t0.00\t0.00\t0.00\t0.00\t0.00\n",
      "Predictd List:\n",
      "-0.00\t1.00\t2.00\t3.01\t4.00\t0.00\t-0.00\t0.00\t0.00\t0.00\n",
      "\n",
      "\n",
      "An Out Of Distribution Input:\n",
      "0.50\t0.60\t0.70\t0.80\t0.90\t0.95\t0.97\t-1.00\t-1.00\t-1.00\n",
      "True Target:\n",
      "0.00\t1.00\t2.00\t3.00\t4.00\t5.00\t6.00\t0.00\t0.00\t0.00\n",
      "Predictd List:\n",
      "0.00\t0.99\t2.04\t3.03\t3.92\t5.04\t6.03\t0.00\t0.00\t0.00\n",
      "\n",
      "\n",
      "A Pathological Input:\n",
      "tensor([ 0.9000,  0.9500,  0.9750,  0.9900,  0.9950,  0.9975,  0.9999, -1.0000,\n",
      "        -1.0000, -1.0000])\n",
      "True Target:\n",
      "0.00\t1.00\t2.00\t3.00\t4.00\t5.00\t6.00\t0.00\t0.00\t0.00\n",
      "Predictdd List:\n",
      "0.01\t1.08\t2.44\t3.21\t4.47\t5.00\t5.68\t0.00\t0.00\t0.00\n"
     ]
    }
   ],
   "source": [
    "inp, t = next(iter(trainLoader))\n",
    "def parseTensor(tensor):\n",
    "    return '\\t'.join([f\"{x.item():.2f}\" for x in tensor])\n",
    "print(\"Input List:\")\n",
    "print(parseTensor(inp[0]))\n",
    "print(\"True Target List:\")\n",
    "print(parseTensor(t[0]))\n",
    "print(\"Predicted List:\")\n",
    "print(parseTensor(model(inp.to(device))[0]))\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "inp = torch.tensor([[0.1, 0.2, 0.3, 0.4, 0.5, -1, -1, -1, -1, -1]])\n",
    "trueTarget = torch.tensor(genAnswerKey(inp[0]))\n",
    "print(\"An In Distribution Input:\")\n",
    "print(parseTensor(inp[0]))\n",
    "print(\"True Target:\")\n",
    "print(parseTensor(trueTarget))\n",
    "print(\"Predictd List:\")\n",
    "print(parseTensor(model(inp.to(device))[0]))\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "inp = torch.tensor([[0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.97, -1, -1, -1]])\n",
    "trueTarget = torch.tensor(genAnswerKey(inp[0]))\n",
    "print(\"An Out Of Distribution Input:\")\n",
    "print(parseTensor(inp[0]))\n",
    "print(\"True Target:\")\n",
    "print(parseTensor(trueTarget))\n",
    "print(\"Predictd List:\")\n",
    "print(parseTensor(model(inp.to(device))[0]))\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "inp = torch.tensor([[0.9, 0.95, 0.975, 0.99, 0.995, 0.9975, 0.9999, -1, -1, -1]])\n",
    "trueTarget = torch.tensor(genAnswerKey(inp[0]))\n",
    "print(\"A Pathological Input:\")\n",
    "print(inp[0])\n",
    "print(\"True Target:\")\n",
    "print(parseTensor(trueTarget))\n",
    "print(\"Predictdd List:\")\n",
    "print(parseTensor(model(inp.to(device))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "id": "1434b4d6-cd2a-4957-8578-03cf47f19de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0     Train Loss 1.132    TestLoss 1.04     \n",
      "Epoch  50    Train Loss 0.495    TestLoss 0.52     \n",
      "Epoch  100   Train Loss 0.469    TestLoss 0.5      \n",
      "Epoch  150   Train Loss 0.436    TestLoss 0.49     \n",
      "Epoch  200   Train Loss 0.425    TestLoss 0.48     \n",
      "Epoch  250   Train Loss 0.404    TestLoss 0.48     \n",
      "Epoch  300   Train Loss 0.391    TestLoss 0.48     \n",
      "Epoch  350   Train Loss 0.375    TestLoss 0.48     \n",
      "Epoch  400   Train Loss 0.365    TestLoss 0.47     \n",
      "Epoch  450   Train Loss 0.358    TestLoss 0.47     \n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "class DumbModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dumbLin = nn.Sequential(\n",
    "            nn.Linear(10, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64,8),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(8,10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.dumbLin(x)\n",
    "\n",
    "n_epochs = 500\n",
    "initialLearningRate = 0.03\n",
    "\n",
    "dumbModel = DumbModel()\n",
    "\n",
    "optimizer = torch.optim.Adam(dumbModel.parameters(), lr = initialLearningRate)\n",
    "scheduler = CosineAnnealingLR(optimizer, n_epochs)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dumbModel.to(device)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    runningLoss = 0\n",
    "    for inputTensor, targetTensor in trainLoader:\n",
    "        dumbModel.zero_grad()\n",
    "        pred = dumbModel(inputTensor.to(device))\n",
    "        loss = criterion(pred, targetTensor.to(device))\n",
    "        loss.backward()\n",
    "        runningLoss += loss.item() * inputTensor.shape[0]\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    runningTestLoss = 0\n",
    "    for inputTensor, targetTensor in testLoader:\n",
    "        with torch.no_grad():\n",
    "            pred = dumbModel(inputTensor.to(device))\n",
    "            loss = criterion(pred, targetTensor.to(device))\n",
    "            runningTestLoss += loss.item() * inputTensor.shape[0]\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch:< 7}Train Loss{round(runningLoss / len(trainDataset), 3):< 10}TestLoss{round(runningTestLoss / len(testDataset), 2):< 10}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "id": "269c1a0a-dc1c-459a-9395-bb0cf8d92efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input List:\n",
      "0.53\t-1.00\t-1.00\t-1.00\t-1.00\t0.72\t-1.00\t-1.00\t-1.00\t0.04\n",
      "True Target List:\n",
      "1.00\t0.00\t0.00\t0.00\t0.00\t2.00\t0.00\t0.00\t0.00\t0.00\n",
      "Predicted List:\n",
      "0.96\t-0.00\t0.02\t-0.00\t-0.00\t-0.01\t0.00\t0.00\t-0.00\t-0.12\n",
      "\n",
      "\n",
      "An In Distribution Input:\n",
      "0.10\t0.20\t0.30\t0.40\t0.50\t-1.00\t-1.00\t-1.00\t-1.00\t-1.00\n",
      "True Target:\n",
      "0.00\t1.00\t2.00\t3.00\t4.00\t0.00\t0.00\t0.00\t0.00\t0.00\n",
      "Predictd List:\n",
      "0.28\t0.00\t2.32\t2.79\t3.02\t0.58\t-0.00\t-0.00\t-0.00\t0.00\n",
      "\n",
      "\n",
      "An Out Of Distribution Input:\n",
      "0.50\t0.60\t0.70\t0.80\t0.90\t0.95\t0.97\t-1.00\t-1.00\t-1.00\n",
      "True Target:\n",
      "0.00\t1.00\t2.00\t3.00\t4.00\t5.00\t6.00\t0.00\t0.00\t0.00\n",
      "Predictd List:\n",
      "0.63\t0.00\t3.62\t1.76\t3.34\t1.18\t4.19\t0.00\t0.00\t0.00\n",
      "\n",
      "\n",
      "A Pathological Input:\n",
      "tensor([ 0.9000,  0.9500,  0.9750,  0.9900,  0.9950,  0.9975,  0.9999, -1.0000,\n",
      "        -1.0000, -1.0000])\n",
      "True Target:\n",
      "0.00\t1.00\t2.00\t3.00\t4.00\t5.00\t6.00\t0.00\t0.00\t0.00\n",
      "Predictdd List:\n",
      "2.46\t0.00\t4.07\t3.02\t2.88\t1.00\t3.36\t0.00\t0.00\t0.00\n"
     ]
    }
   ],
   "source": [
    "inp, t = next(iter(trainLoader))\n",
    "def parseTensor(tensor):\n",
    "    return '\\t'.join([f\"{x.item():.2f}\" for x in tensor])\n",
    "print(\"Input List:\")\n",
    "print(parseTensor(inp[0]))\n",
    "print(\"True Target List:\")\n",
    "print(parseTensor(t[0]))\n",
    "print(\"Predicted List:\")\n",
    "print(parseTensor(dumbModel(inp.to(device))[0]))\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "inp = torch.tensor([[0.1, 0.2, 0.3, 0.4, 0.5, -1, -1, -1, -1, -1]])\n",
    "trueTarget = torch.tensor(genAnswerKey(inp[0]))\n",
    "print(\"An In Distribution Input:\")\n",
    "print(parseTensor(inp[0]))\n",
    "print(\"True Target:\")\n",
    "print(parseTensor(trueTarget))\n",
    "print(\"Predictd List:\")\n",
    "print(parseTensor(dumbModel(inp.to(device))[0]))\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "inp = torch.tensor([[0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.97, -1, -1, -1]])\n",
    "trueTarget = torch.tensor(genAnswerKey(inp[0]))\n",
    "print(\"An Out Of Distribution Input:\")\n",
    "print(parseTensor(inp[0]))\n",
    "print(\"True Target:\")\n",
    "print(parseTensor(trueTarget))\n",
    "print(\"Predictd List:\")\n",
    "print(parseTensor(dumbModel(inp.to(device))[0]))\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "inp = torch.tensor([[0.9, 0.95, 0.975, 0.99, 0.995, 0.9975, 0.9999, -1, -1, -1]])\n",
    "trueTarget = torch.tensor(genAnswerKey(inp[0]))\n",
    "print(\"A Pathological Input:\")\n",
    "print(inp[0])\n",
    "print(\"True Target:\")\n",
    "print(parseTensor(trueTarget))\n",
    "print(\"Predictdd List:\")\n",
    "print(parseTensor(dumbModel(inp.to(device))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bae1ad-4186-4998-b363-58183932b2f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb50a9-b590-43c1-b23e-4a0135804612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
